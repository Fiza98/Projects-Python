#!/usr/bin/env python
# coding: utf-8

# **Naïve Bayes Classifier in python (Numeric data)**
# Macam MLR, Output = Y/N (numerical or categorical)

# A survey has been conducted to collect the information
# about a person will go to work by car or LRT according to
# their age and salary per month.

# 1.Gaussian NB: It should be used for features in decimal form , continuous. GNB assumes features to follow a normal distribution.
# 
# 2.MultiNomial NB: Its is used when we have discrete data (e.g. movie ratings ranging 1 and 5 as each rating will have certain frequency to represent),
# It should be used for the features with discrete values like word count 1,2,3...
# 
# 3.Bernoulli NB: It should be used for features with binary or boolean values like True/False or 0/1.

# In[1]:


#Naïve Bayes Classifier
#import libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

#import dataset
ds=pd.read_csv ('LRT_car.csv')
ds.head()


# In[2]:


ds.info()


# In[3]:


x=ds.iloc[:,:-1].values
y=ds.iloc [:,2].values
#print(x)


# In[4]:


#Naïve Bayes Classifier Step 2
#splitting the dataset into training and test sets
from sklearn.model_selection import train_test_split
x_train , x_test , y_train , y_test = train_test_split (x, y, test_size = 0.25, random_state =0)


# In[5]:


#Naïve Bayes Classifier Step 3 (optional or depends) - to standardize
#Feature Scaling
from sklearn.preprocessing import StandardScaler
sc_x=StandardScaler()
x_train=sc_x.fit_transform(x_train)
x_test=sc_x.transform(x_test)


# In[6]:


##Naïve Bayes Classifier Step 4
#fitting Naïve Bayes Classifier to the training set
#use GaussianNB sbb input all numerical
from sklearn.naive_bayes import GaussianNB
classifier=GaussianNB()
classifier.fit(x_train,y_train)


# In[7]:


#Naïve Bayes Classifier Step 5
#Predict the test set results
y_pred=classifier.predict(x_test)


# In[8]:


y_test, y_pred
#boleh cek berapa error, nak error low, high accuracy


# In[9]:


#Step 6 - rujuk slide 96 belah bawah kanan
#Making Confusion Matrix to check whether the test set gives the correct result
from sklearn.metrics import confusion_matrix
cm=confusion_matrix(y_test,y_pred)
cm
#47+43=90 predicted correctly, 90% accuracy, already satisfied


# In[10]:


#Predict a person with salary RM6000 and 35 years old will go to work by LRT or car?
#Predict using classifier
xP=sc_x.transform ([[6000,35]])
print(xP)


# In[11]:


#classifier predict for [6000,35]
prediction= classifier.predict(xP)
print(prediction)
#answer:LRT


# Conclusion: The person with salary RM6000 and 35 years old will go to work by LRT.

# **Classification: Naïve Bayes Classifier Example (Not Numerical)**
# macam MLR, categorical data, output = Y/N

# In[12]:


#import libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

#import dataset
ds=pd.read_csv ('play.csv')
ds.head()


# In[13]:


ds.info()


# In[14]:


x=ds.iloc[:,:-1].values
y=ds.iloc [:,4].values
#case ni taknak split train n test sbb data sikit nnti jadi no accuracy.


# In[15]:


#set is to get unique data, then make it into list follow by sort
#after sort, data in sequence 0,1,2
i=0
for i in range(0,4):
    d=list(set(ds.iloc[:, i].values))
    d.sort()
    print(i,d)
    i=i+1


# In[16]:


#Handling or Encode categorical variables
from sklearn.preprocessing import LabelEncoder
labelencoder_x= LabelEncoder()
x[:, 0] =labelencoder_x.fit_transform(x[:,0])
x[:, 1] =labelencoder_x.fit_transform(x[:,1])
x[:, 2] =labelencoder_x.fit_transform(x[:,2])
x[:, 3] =labelencoder_x.fit_transform(x[:,3])
print(x)
#lepas print baru kita boleh identify, 0,1,2 tu merujuk kepada mana, refer to original table


# In[17]:


#splitting the dataset into training and test sets
from sklearn.model_selection import train_test_split
x_train , x_test , y_train , y_test = train_test_split (x, y, test_size = 0.2, random_state =0)


# In[18]:


#STEP 3
#fitting Naïve Bayes Classifier to the dataset
#we dont use GaussianNB
from sklearn.naive_bayes import BernoulliNB
classifier=BernoulliNB()
classifier=classifier.fit(x,y)


# In[19]:


#Preidct the test set results
y_pred=classifier.predict(x_test)
y_test, y_pred


# In[20]:


#STEP 4
#Predict using classifier
#prediction=classifier.predict ([[1, 0, 0, 0]]), taknak letak decimal pun okay
prediction=classifier.predict ([[1., 0., 0., 0.]])#sunny day,cool temperature, humidity high, windy false
print(prediction)


# Conclusion: cannot play

#!/usr/bin/env python
# coding: utf-8

# **LOGISTIC REGRESSION - MOST ACCURATE**
# 
# Macam MLR, cuma data output dia numerical, naive bayes output dia categorical

# Given a set of data (in
# buy_car.csv ) containing information whether a person will
# buy a car base on their monthly salary and age.

# In[1]:


#Step 1
#import libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

#import dataset
ds=pd .read_csv ('buy_car.csv')
ds.head()


# In[2]:


ds.info()


# In[3]:


x=ds.iloc [:,:-1].values
y=ds.iloc [:,2].values


# In[4]:


#Step 2
#splitting the dataset into training and test sets
from sklearn.model_selection import train_test_split
x_train , x_test , y_train , y_test = train_test_split (x, y, test_size = 0.25,
random_state = 0)


# In[5]:


#STEP 3 (OPTIONAL) FEATURE SCALLING IS FOR STANDARDIZE THE DATA
#Feature Scaling
from sklearn.preprocessing import StandardScaler
sc_x = StandardScaler()
x_train=sc_x.fit_transform(x_train)
x_test=sc_x.transform(x_test) #dont need fit 


# In[6]:


#STEP 4
#fitting Logistic regression to the training set 
from sklearn.linear_model import LogisticRegression
classifier=LogisticRegression(random_state=0)
classifier.fit(x_train,y_train)


# In[7]:


#STEP 5
#Predict the test set results
y_pred=classifier.predict(x_test)
y_pred,y_test


# In[8]:


#Making Confusion Matrix to check whether the test set gives the correct result
from sklearn.metrics import confusion_matrix
cm= confusion_matrix (y_test,y_pred)
cm


# In[9]:


#will a person with salary = 15000 and age = 21 buy the car?
y_pred_new=classifier.predict(sc_x.transform([[15000,21]]))
print(y_pred_new)


# Conclusion: The person with salary = 15000 and age = 21 will not buy the car

# **KNN - K Nearest Neighbour**

# In[10]:


#KNN-Step 1
#import libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

#import dataset
ds=pd .read_csv ('buy_car.csv')
ds.head()


# In[11]:


ds.info()


# In[12]:


#x=ds.iloc [:,[0,1]].values
x=ds.iloc [:,:-1].values
y=ds.iloc [:,2].values


# In[13]:


#KNN Step 2
#splitting the dataset into training and test sets
from sklearn.model_selection import train_test_split
x_train , x_test , y_train , y_test = train_test_split (x, y, test_size = 0.25,
random_state = 0)


# In[14]:


#KNN-Step 3
#Feature Scaling for KNN is IMPORTANT!!! not optional okay
from sklearn.preprocessing import StandardScaler
sc_x=StandardScaler()
x_train= sc_x.fit_transform(x_train)
x_test=sc_x.transform(x_test)


# In[15]:


#KNN-Step 4
#fitting KNN classifier to the training set
#USE KNeighborsClassifier
from sklearn.neighbors import KNeighborsClassifier
classifier=KNeighborsClassifier(n_neighbors=5, metric='minkowski',p=2) 
#refer to using Euclidean distance
#P=2 REFER TO MINWOSKI, P=1 REFERS TO MANHATTAN?

classifier.fit(x_train,y_train)
#p : int, default=2 Power parameter for the Minkowski metric (either 1 or 2)
#can try either one of this hyper parameter
#manhattan distance is...tgk ss
#


# In[16]:


#KNN-Step 5
#Predict the test set results
y_pred=classifier.predict(x_test)


# In[17]:


#KNN-Step 6
#Making Confusion Matrix to check whether the test set gives the correct result
from sklearn.metrics import confusion_matrix
cm= confusion_matrix(y_test,y_pred)
cm
#92% accuracy (47+45)


# In[18]:


#KNN Prediction
#Will a person with with salary = 15000 and age = 21 buy the car??
y_pred_new=classifier.predict(sc_x.transform(np.array ([[15000,21]])))
print(y_pred_new)


# Conclusion: The person with with salary = 15000 and age = 21 will buy the car

# In[19]:


#Finding accuracy
from sklearn.metrics import accuracy_score
AR=accuracy_score(y_test,y_pred)
print(AR)


# **SVM -Support Vector Machine(SVM)**

# In[1]:


#SVM-Step 1
#import libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

#import dataset
ds=pd.read_csv ('buy_car.csv')
x=ds.iloc [:,:-1].values
y=ds.iloc [:,2].values


# In[2]:


#SVM-Step 2
#splitting the dataset into training and test sets
from sklearn.model_selection import train_test_split
x_train , x_test , y_train , y_test = train_test_split (x, y, test_size = 0.25,
random_state = 0)


# In[3]:


#SVM-Step 3
#Feature Scaling
from sklearn.preprocessing import StandardScaler
sc_x =StandardScaler()
x_train= sc_x.fit_transform (x_train)
x_test =sc_x.transform (x_test)


# In[4]:


#SVM-Step 4
#fitting SVM classifier to the training set
from sklearn.svm import SVC
classifier= SVC(kernel='linear',random_state =0)#if use linear, final no. if use rbf final will be yes
classifier.fit(x_train,y_train)


# In[5]:


#SVM-Step 5
#Predict the test set results
y_pred = classifier.predict(x_test)


# In[6]:


y_pred


# In[7]:


#SVM-Step 6
#Making Confusion Matrix to check whether the test set gives thecorrect result
from sklearn.metrics import confusion_matrix
cm= confusion_matrix (y_test,y_pred)
cm


# In[8]:


#SVM-Prediction
#Will a person with salary = 15000 and age = 21 buy the car?
y_pred_new=classifier.predict(sc_x.transform(np.array([[15000,21]])))
print(y_pred_new)


# **Conclusion: The person with salary = 15000 and age = 21 will not buy the car**

# TRY TO CHANGE LINEAR TO RBF

# In[28]:


#Kernel SVM Step 4
#try to change linear to rbf
#fitting Kernel SVM classifier to the training set
from sklearn.svm import SVC
classifier= SVC(kernel='rbf', random_state =0)
classifier.fit(x_train,y_train)


# In[29]:


#Finding accuracy
from sklearn.metrics import accuracy_score
AR=accuracy_score(y_test,y_pred)
print(AR)

#!/usr/bin/env python
# coding: utf-8

# **DECISION TREE**

# In[1]:


#Decision Tree Classification-Step 1
#import libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

#import dataset
ds=pd .read_csv ('buy_car.csv')
ds.head()


# In[2]:


ds.info()


# In[3]:


x=ds.iloc [:,:-1].values
y=ds.iloc [:,2].values


# In[4]:


# Decision Tree Classification-Step 2
#splitting the dataset into training and test sets
from sklearn.model_selection import train_test_split
x_train , x_test , y_train , y_test = train_test_split (x, y, test_size = 0.25, random_state = 0)


# In[5]:


# Decision Tree Classification-Step 3 (no need)
#Feature Scaling
from sklearn.preprocessing import StandardScaler
sc_x =StandardScaler()
x_train =sc_x.fit_transform(x_train)
x_test =sc_x.transform(x_test)


# In[6]:


# Decision Tree Classification-Step 4
#fitting Decision Tree Classification classifier to the training set
from sklearn.tree import DecisionTreeClassifier
classifier=DecisionTreeClassifier ( criterion='entropy', random_state = 0)
classifier.fit(x_train,y_train)
#Entropy is an information theory metric that measures the impurity or uncertainty in a group of observations.
#The Gini Index or Gini Impurity is calculated by subtracting the sum of the squared probabilities of each class from one


# In[7]:


# Decision Tree Classification-Step 5
#Predict the test set results
y_pred = classifier.predict(x_test)


# In[8]:


# Decision Tree Classification-Step 6
#Making Confusion Matrix to check whether the test set gives the correct result
from sklearn.metrics import confusion_matrix
cm= confusion_matrix (y_test,y_pred)
cm
#90% accuracy (46+44)


# In[9]:


#Decision Tree Classification Prediction
#Will a person with salary = 15000 and age = 21 buy the car?
y_pred_new=classifier.predict(sc_x.transform(np.array([[15000,21]])))
print(y_pred_new)


# **Conclusion: The  person with salary = 15000 and age = 21 will not buy the car**

# In[10]:


#Finding accuracy
from sklearn.metrics import accuracy_score
AR=accuracy_score(y_test,y_pred)
print(AR)


# In[11]:


#plot tree
from sklearn import tree
from matplotlib import pyplot as plt

plt.figure(figsize=(15,15))  # set plot size (denoted in inches)
tree.plot_tree(classifier.fit(x_train,y_train),fontsize=12)
plt.show()


# **RANDOM FOREST**

# In[12]:


#Random Forest Classification-Step 1
#import libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

#import dataset
ds=pd.read_csv ('buy_car.csv')
ds.head()


# In[13]:


ds.info()


# In[14]:


#x=ds.iloc [:,[0,1]].values
x=ds.iloc [:,:-1].values
y=ds.iloc [:,2].values


# In[15]:


# Random Forest Classification-Step 2
#splitting the dataset into training and test sets
from sklearn.model_selection import train_test_split
x_train , x_test , y_train , y_test = train_test_split (x, y, test_size = 0.25, random_state = 0)


# In[16]:


# Random Forest Classification-Step 3 (no need)
#Feature Scaling
from sklearn.preprocessing import StandardScaler
sc_x =StandardScaler()
x_train= sc_x.fit_transform (x_train)
x_test= sc_x.transform (x_test)


# In[17]:


# Random Forest Classification-Step 4
#fitting Random Forest Classification classifier to the training set
from sklearn.ensemble import RandomForestClassifier
classifier=RandomForestClassifier (n_estimators =100,criterion='entropy', random_state =0)
classifier.fit(x_train,y_train)
#Entropy is an information theory metric that measures the impurity or uncertainty in a group of observations.
#The Gini Index or Gini Impurity is calculated by subtracting the sum of the squared probabilities of each class from one


# In[18]:


# Random Forest Classification-Step 5
#Predict the test set results
y_pred =classifier.predict (x_test)


# In[19]:


# Random Forest Classification-Step 6
#Making Confusion Matrix to check whether the test set gives the correct result
from sklearn.metrics import confusion_matrix
cm= confusion_matrix (y_test,y_pred)
cm
#90% accuracy


# In[20]:


#Finding accuracy
from sklearn.metrics import accuracy_score
AR=accuracy_score(y_test,y_pred)
print(AR)


# In[21]:


#Random Forest Classification Prediction
#Will a person with salary = 15000 and age = 21 buy the car?
y_pred_new=classifier.predict (sc_x.transform (np.array ([[15000,21]])))
print(y_pred_new)


# **Conclusion: The person with salary = 15000 and age = 21 will not buy the car**
